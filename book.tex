\documentclass[openany]{book}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{framed} % frames around example problems
\usepackage{relsize} % larger math font size in one place
\usepackage{listings} % source code listings
\usepackage{courier} % bold monospace
% \usepackage{ctex} % Chinese characters, takes too long to compile for now

% lstlisting settings:
\lstset{language=C++}
\lstset{morekeywords={alignas,alignof}}
\lstset{showstringspaces=false}
\lstset{breaklines=true}
\lstset{tabsize=2}
\lstset{frame=single}
\lstset{basicstyle=\small\ttfamily} % or \footnotesize?
% \lstset{commentstyle=\normalfont\itshape}

% Thanks to:
% Dandan for AVX-512 and bouncing ideas
% problemsetters
% bunch of people for the float modmul

% only gcc (pragmas + what judges use)
% measurements on i5-4200U (old laptop, has AVX2)
% repo with source code

\title{
    Down With Constant Factors! \\
    \large An introduction to SIMD and low-level optimization in competitive programming
}
\author{Simon Lindholm}
\date{\today}

\begin{document}

\maketitle

\tableofcontents

\chapter{Introduction}
only focus on integer instructions
fun way of applying SIMD, not just for competitive programmers
why not multithreading (simd is ``free performance'')
goals:
- optimizing when solution gets TLE
- getting away with additional log factors
- sqrt decomp
- quadratic
- having fun
- useful outside of competitive programming
only going to cover constant factors, mostly problem-independent

\part{General Optimization Tips}
\chapter{Workflow}
- language
- compiler flags (-O2, -Ofast, ASAN/UBSAN)
- do the math
- first make it work
- algorithmic optimizations (pruning, breaking early, special cases, symmetry, shaving off factors of 2)
- test locally. if it takes 20 seconds algorithmic optimizations are needed and you shouldn't submit
- concentrate on hotspots. 100ms usually doesn't matter
- profilers (gdb, perf flamecharts)
- looking at assembly (-S -fno-asynchronous-unwind-tables or godbolt)
\chapter{Input/output}
- cin, cout
- \verb@.sync_with_stdio(0)@
- .tie(0)
- scanf, printf
- \verb@{get,put}char_unlocked@
- custom IO
- syscall overhead and variance between judges
\chapter{The CPU pipeline}
- latency, throughput
- example: linked list vs vector iteration (computing a sum)
- branch prediction
- example: filtered sum vs ?:
- register pressure
- instruction latencies/throughput: int arithmetic (addition/multiplication/division, smaller vs larger datatypes), float arithmetic (f32, f64), move, memory access
\chapter{What can the compiler do for you?}
- instruction scheduling
- constant folding
- LICM, CSE
- limited alias analysis (\verb@__restrict__@)
- inlining (-O2 vs -O3)
- loop unrolling (pragma, and useless with modern processors because it's not the bottleneck)
- autovectorization (O3 and target pragmas)
- what can it not do? memory allocations, data formats, loop order (except icc), float associativity (unless -Ofast), modular arithmetic associativity
- divisions by constant vs non-constant
- example: factorial
\chapter{Dealing with memory}
- overhead of malloc (perf \& size)
- overhead of free (\verb@exit(0)@)
- vector vs array
- \verb@vector<vector<vector<int>>>@ vs \verb@vector<vector<array<int, 2>>>@ vs \verb@array<vector<vector<int>>, 2>@
- global variables
- memory access (memory use matters, L1/L2/L3/RAM, indirection)
- organizing memory, e.g. transposed matrices
- example: matrix multiplication
- example: dp with bad access order
- reusing memory in DP
\chapter{Example: dynamic programming}
- starting with a memoized version
- memoization: pros and cons
- write 5-line python generator, \verb@./a.out <input | sha1sum@
- call it in a loop
- strip out memoization parts
- extract out from function
- transposing arrays
- help out alias analysis?
\chapter{Working with bits}
- bit hacks (\url{https://graphics.stanford.edu/~seander/bithacks.html})
- bitsets (\verb@_Find_first@, \verb@_Find_next@, \verb@count()@)
- compiler intrinsics: \verb@__builtin_clzll@, \verb@popcountll@
- example: ?? gauss elim?

\part{SIMD}

Assorted:
- more pshufb
- (FFT? nah)
- exercises:
 * quicksort/mergesort
 * base64 encoding/decoding (tips: \verb@_mm_shuffle_epi8@, shifts, masks, compares, saturated arithmetic)

\chapter{Introduction to SIMD}
single instruction multiple data
focus on x86
intrinsics
intrinsics guide
manual assembly
auto-vectorization
example: sum (+unrolling to avoid latency-bound)
includes
look at asm
auto-vectorizing: how to (pragmas), limitations
SSE/AVX/AVX-512 (judge hardware, crashes)
aligned memory (crashes, \verb@_mm_loadu_si128@)
smaller datatypes
performance
what is simd good at
- vertical operations
what is it not good at
- memory access
- horizontal reduction
memory layout

\chapter{Reference}
list of useful instructions for SSE, AVX, AVX-512 with latency/throughput
missing instructions:
- cross-lane shuffles
- cross-lane shifts
- dynamic shifts
- int division
- small-width operations (multiplication)
- large-width operations (multiplication)

\chapter{Example: Robots}
\url{https://codeforces.com/contest/575/problem/I}
% concepts: auto-vectorization, soa, small datatypes, tight inner loops
$N \le 5000, Q \le 10^5$. TL 1.5s. Queries of 2 kinds:
- \verb@1 d x y l@: add an axis-aligned triangle at $(x,y)$, oriented depending on $d \in \{1,2,3,4\}$
- \verb@2 x y@: count how many triangles cover the point $(x, y)$
naive first implementation runs in 10s
unpredictable branches, \&\& -> \& gives 5s
no auto-vectorization despite pragmas
SoA and small data types, 0m1.748s
look at asm and iterately improve
finally 0m0.840s (0.710 with AVX2)
manual SIMD for comparison, 0.8s

\chapter{Example: Binomial Coefficients}
% concepts: modulo, latency
\section{Modulo powers of two}
\begin{framed}
\noindent
(Russian training camp) Given $N, K$ ($0 \le K \le N \le 10^{18}$), compute
\[
\binom N K = \frac{N!}{K!(N-K)!} \pmod {2^{32}}.
\]
Time limit: 1 second.
\end{framed}
This problem seems to call for some extension of Lucas' theorem to prime powers... but let's ignore that and instead focus our attention on how to compute factorials quickly.

\newcommand{\odd}[1]{#1_\text{odd}}
\newcommand{\even}[1]{#1_\text{even}}

We can't compute $N!$, $K!$ and $(N-K)!$ straight off modulo $2^{32}$ and then do modular division, because they will be divisible by large powers of two, and division by zero is impossible.
Instead, let's take each factorial and write it as $x! = \odd{x}2^{\even{x}}$. Then
\[
\frac{N!}{K!(N-K)!} =
\frac{\odd N}{\odd K \odd{(N-K)}} \cdot \mathlarger{2^{\even N - \even K - \even{(N-K)}}},
\]

and the $\odd x$ parts can be computed mod $2^{32}$ instead of requiring arbitrary precision integers.
We can get formulas for $\odd x$ and $\even x$ by expanding $x!$ as follows: \\
\begin{tabular}{cccccccccc}
$x!$&$=$&          &$1$& $\cdot 2$ &$\cdot 3$ &$\cdot 4$ &$\cdot 5$ & $\cdot 6$ & $\cdot \ldots$ \\
    &$=$&          &$1$&           &$\cdot 3$ &          &$\cdot 5$ &           & $\cdot \ldots$ \\
    &   &$\cdot 2$(&   & $1$       &          &          &          & $\cdot 3$ & $\cdot \ldots$)\\
    &   &$\cdot 4$(&   &           &          &  $1$     &          &           & $\cdot \ldots$)\\
    &   &$\cdot \ldots$ &
\end{tabular}
\\
$\even x$ gets an easy formula: $\sum_{k=1}^\infty \lfloor x / 2^k \rfloor$.
For $\odd x$, we get a decomposition into a number of odd double factorials $k!! = 1 \cdot 3 \cdot 5 \cdot \ldots \cdot k$.

As it turns out, $1 \cdot 3 \cdot \ldots \cdot (2^{32}-1) = 1 \pmod{2^{32}}$, so $k!! = (k \mod 2^{32})!! \pmod{2^{32}}$.

Now, we could try to compute all the double factorials with bruteforce, but it would be a bit too costly -- there are up to $3 \cdot \log_2{10^{18}}$ of them.
But we can be a bit smarter, by reusing parts of the products.
Let's partition the numbers $1, 3, \dots, 2^{32}-1$ into intervals that affect the same double factorials.
Then we get an amortized runtime of $2^{31}$ multiplications, which should be totally doable in 1 second.
(Note that arithmetic mod $2^{32}$ is fast, because it can make use of integer overflow.)
Here is how it looks in code:

\begin{lstlisting}
typedef uint32_t u32;

u32 modpow(u32 a, ll e) {
	if (e == 0) return 1;
	u32 x = modpow(a * a, e >> 1);
	return e & 1 ? x * a : x;
}

u32 solve(ll N, ll K) {
	ll trailingZeroes = 0;
	map<u32, int> ivs;
	int accMult = 0;
	rep(i,0,3) {
		ll x = (i == 0 ? N : i == 1 ? K : N-K);
		int mult = (i == 0 ? 1 : -1);
		while (x > 0) {
			// Include the product (1 * 3 * ... * (x % 2^32))
			// in the answer, 'mult' times.
			ivs[(u32)x + 1] += mult;
			accMult += mult;
			x /= 2;
			trailingZeroes += x * mult;
		}
	}

	u32 cur = 0, res = 1, resdiv = 1;
	for (auto pa : ivs) {
		u32 lim = pa.first, ilim = lim / 2;
		// The odd numbers in the range [last lim, lim) get
		// included 'accMult' times in the answer -- 'pa.second'
		// times for the interval ending at 'lim', and also for
		// all larger intervals. We divide the loop counter by 2
		// to avoid potential overflows.
		u32 prod = 1;
		for (; cur < ilim; cur++)
			prod *= cur * 2 + 1;
		if (accMult > 0) res *= modpow(prod, accMult);
		else resdiv *= modpow(prod, -accMult);
		accMult -= pa.second;
	}

	res *= modpow(2, trailingZeroes);
	res *= modpow(resdiv, (1LL << 31) - 1);
	return res;
}
\end{lstlisting}

Running this locally on worst-case input (e.g. $n = 2^{59}-2$, $k$ randomly keyboard mashed, to trigger a loop over the full range $[1, 2^{32}-1]$) results in a time of 2.601s. Not great...

Let's add some auto-vectorization (SSE 4.1, to match the judge's hardware):
\begin{lstlisting}
#pragma GCC optimize ("O3")
#pragma GCC target ("sse4.1")
\end{lstlisting}

New time: 2.141s. That's better, but not good. What's going on?
If we experiment locally by changing \verb@sse4.1@ to \verb@avx2@ it gives a time of 1.065s.
This is exactly what we would expect if the code was auto-vectorized and the vector width doubled with AVX2.
A hypothesis we can make from this is that the code \emph{was} auto-vectorized into operating on 4 u32's at once, but that the loop is completely latency-bound, and the added latency of \verb@pmulld@ (\verb@_mm_mullo_epi32@) over \verb@imul@ (normal multiplication) compensated for the increased throughput.
(Agner's instruction tables seem to agree with this hypothesis -- imul is listed as having 3 cycles of latency on Skylake, while pmulld has 10.)
If that's the case, we should be able to make it throughput-bound and increase performance by keeping a bunch of simultaneous loop counters.
Let's try it:

\begin{lstlisting}
const int PAR = 8;
u32 subprod[PAR] = {1,1,1,1,1,1,1,1};
u32 cur2 = cur * 2 + 1;
for (; cur + PAR < ilim; cur += PAR, cur2 += PAR*2) {
	rep(i,0,PAR) subprod[i] *= cur2 + 2*i;
}
u32 prod = 1;
rep(i,0,PAR) prod *= subprod[i];
for (; cur < ilim; cur++) {
	prod *= cur * 2 + 1;
}
\end{lstlisting}

0.434s! That's better, and enough to get Accepted on the problem.
With an \verb@avx2@ target pragma it goes down further to 0.225s.
In fact, even with auto-vectorization disabled this trick helps performance, giving 0.853s.

For fun we can also try writing the loop manually using intrinsics:
\begin{lstlisting}
#pragma GCC optimize ("unroll-loops")
typedef __m128i M;
...

const int PAR = 8;
M madd = _mm_set1_epi32(PAR * 8);
M msubprod[PAR], mcur2[PAR];
rep(i,0,PAR) {
	msubprod[i] = _mm_set1_epi32(1);
	mcur2[i] = _mm_setr_epi32(
	    2*cur + 8*i + 1, 2*cur + 8*i + 3,
	    2*cur + 8*i + 5, 2*cur + 8*i + 7);
}
for (; cur + PAR * 4 < ilim; cur += PAR * 4) {
	rep(i,0,PAR) {
		msubprod[i] = _mm_mullo_epi32(msubprod[i], mcur2[i]);
		mcur2[i] = _mm_add_epi32(mcur2[i], madd);
	}
}
u32 prod = 1;
union {
	M mprod;
	u32 parts[4];
} u;
u.mprod = _mm_set1_epi32(1);
rep(i,0,PAR) u.mprod = _mm_mullo_epi32(u.mprod, msubprod[i]);
rep(i,0,4) prod *= u.parts[i];
for (; cur < ilim; cur++) {
	prod *= cur * 2 + 1;
}
\end{lstlisting}

This gives a runtime of 0.442s; roughly the same as auto-vectorization.

\section{Modulo arbitrary numbers}

Let's make life more difficult on ourselves.

\begin{framed}
\noindent
Given $N, K, M$ ($0 \le K \le N \le 10^{18}$, $1 \le M \le 10^9$), compute
\[
\binom N K = \frac{N!}{K!(N-K)!} \pmod {M}.
\]
Time limit: 1 second.
\end{framed}

Our approach to this will be similar to that of powers of two.
We can start by factoring $M = \prod p^\alpha$, and solve the problem modulo each prime power, piecing the results together using the Chinese remainder theorem.
For each prime power $p^\alpha$, we decompose $N!$, $K!$ and $(N-K)!$ into powers of $p$ and products of ranges $[1, k]$ with numbers divisible by $p$ excluded.
The product of a whole interval $[1, p^\alpha]$ is $1$ if $p = 2, \alpha \ge 3$, else $-1$,\footnote{
This isn't really necessary to the implementation, since we could just as well compute the product of a whole interval using a brute-force loop. But it's a fun excuse to do math.

If $\alpha = 1$, Wilson's theorem says that $(p-1)! = -1 \pmod{p}$, and has an easy proof: rearrange the product $1 \cdot 2 \cdot \ldots \cdot (p-1)$ to group $x$ together with $x^{-1}$; this will get rid of all factors except the ones that are their own inverse. Solving $x = x^{-1}$, or equivalently $x^2 = 1$, yields just two solutions $x = \pm 1$, and so $(p-1)! = 1 \cdot (-1) = -1\pmod{p}$. An easy way of seeing that there aren't more solutions to $x^2 = 1$ is that a degree two polynomial can only have two roots.

For $p = 2$ we can compute the results for $\alpha = 2, 3$ by hand: $\alpha = 2$ gives a product $1 \cdot 3 = -1$, and $\alpha = 3$ gives $1 \cdot 3 \cdot 5 \cdot 7 = 1$. For larger $\alpha$ we can use the Wilson argument together with induction on the fact that $x^2 = 1$ has four solutions $1$, $-1$, $2^{\alpha-1} - 1$, $2^{\alpha-1} + 1$.

For $p > 2$ we can solve $x^2 = 1 \pmod{p^\alpha}$ using Hensel lifting: a root $x$ to the polynomial mod $p$ lifts to exactly one corresponding root mod higher powers of $p$ since the derivative $2x$ is non-zero for $p > 2$. Hence, $\pm 1$ are the only roots to $x^2 = 1 \pmod{p^\alpha}$, and we can apply the Wilson argument.
}
so we can look only at ranges with upper bound less than $p^\alpha$.
Here's an initial attempt:

\begin{lstlisting}
ll euclid(ll a, ll b, ll &x, ll &y) {
	if (b) { ll d = euclid(b, a % b, y, x);
		return y -= a/b * x, d; }
	return x = 1, y = 0, a;
}

ll crt(ll a, ll m, ll b, ll n) {
	if (n > m) swap(a, b), swap(m, n);
	ll x, y, g = euclid(m, n, x, y);
	assert((a - b) % g == 0); // else no solution
	x = (b - a) % n * x % n / g * m + a;
	return x < 0 ? x + m*n/g : x;
}

template<class F>
void factor(ll n, F f) {
	for (ll p = 2; p*p <= n; p++) {
		int a = 0;
		while (n % p == 0) n /= p, a++;
		if (a > 0) f(p, a);
	}
	if (n > 1) f(n, 1);
}

ll modpow(ll a, ll e, ll m) {
	if (e == 0) return 1;
	ll x = modpow(a * a % m, e >> 1, m);
	return e & 1 ? x * a : x;
}

// N choose K modulo p^a
ll solve(ll N, ll K, int p, int a) {
	int mod = 1, acc = 0;
	rep(i,0,a) mod *= p;
	ll trailingZeroes = 0, res = 1, resdiv = 1;
	map<int, int> ivs;
	rep(i,0,3) {
		ll x = (i == 0 ? N : i == 1 ? K : N-K);
		int mult = (i == 0 ? 1 : -1);
		while (x > 0) {
			// Include the product (1 * 2 * ... * x) in the answer,
			// 'mult' times, with numbers divisible by p excluded.
			ll lim = x + 1;
			ivs[(int)(lim % mod)] += mult;
			if (lim / mod % 2 == 1 && (mod == 4 || p > 2))
				res *= -1;
			acc += mult;
			x /= p;
			trailingZeroes += x * mult;
		}
	}

	int cur = 1;
	for (auto pa : ivs) {
		int lim = pa.first;
		ll prod = 1;
		for (; cur < lim; cur++) {
			if (cur % p != 0)
				prod = prod * cur % mod;
		}
		if (acc > 0) res = res * modpow(prod, acc, mod) % mod;
		else resdiv = resdiv * modpow(prod, -acc, mod) % mod;
		acc -= pa.second;
	}

	res = res * modpow(p, trailingZeroes, mod) % mod;
	res = res * modpow(resdiv, mod - mod/p - 1, mod) % mod;
	return res;
}

// N choose K modulo M
ll solve(ll N, ll K, ll M) {
	ll res = 0, prod = 1;
	factor(M, [&](ll p, int a) {
		ll r = solve(N, K, (int)p, a), m = 1;
		rep(i,0,a) m *= p;
		res = crt(res, prod, r, m);
		prod *= m;
	});
	return res;
}

void test() {
	vector<vector<int>> binom(100, vector<int>(100, -1));
	rep(m,1,100) rep(n,0,100) rep(k,0,n+1) {
		if (k == 0 || k == n) binom[n][k] = 1 % m;
		else binom[n][k] = (binom[n-1][k-1] + binom[n-1][k]) % m;
		assert(solve(n, k, m) == binom[n][k]);
	}
}
\end{lstlisting}

Runtime is an abysmal 20.839s, but it works.
Let us focus on the innermost loop:
\begin{lstlisting}
for (; cur < lim; cur++) {
  if (cur % p != 0)
    prod = prod * cur % mod;
}
\end{lstlisting}

There are two things that are slow: the divisibility check by \texttt{p}, and the modular multiplication.
The former would be easy to eliminate by turning the loop into two nested loops, with the innermost looping from $1$ to $p-1$, however, this will make the code messier when we start applying SIMD.
Instead, let's use another trick:

\begin{lstlisting}
u32 pinv = (u32) modpow(p, (1LL << 31) - 1, 1LL << 32);
u32 plim = 0xFFFFFFFF / p;
...

if (p == 2) {
  u32 prod2 = ... product mod 2^32 as before
  prod = prod2 % mod;
} else {
  for (; cur < lim; cur++) {
    if ((u32)cur * pinv > plim)
      prod = prod * cur % mod;
  }
}
\end{lstlisting}

If \verb@cur@ is divisible by $p$, dividing will result in a number in the range $[0, (2^{32}-1) / p]$.
We can compute this result by multiplying by the modular inverse of $p \pmod{2^{32}}$.
On the other hand, if \verb@cur@ is not divisible by $p$, multiplying by the modular inverse of $p$ cannot result in a number in that range, since that multiplication is invertible and already has a pre-image (given by multiplication by $p$).
Hence, \verb@cur % p == 0@ can be replaced by \verb@(u32)cur * pinv <= plim@, multiplication being a much cheaper operation than modulo.

We do need to be careful to treat $p = 2$ specially, since 2 is not invertible mod $2^{32}$.
However, this special-casing will be needed later on anyway, when we start looking into optimizing the modular multiplication.

Performance-wise, this doesn't make much of a difference -- we go from 20.8 seconds to 19.3. It seems likely that we are latency-bound. To avoid this latency bottleneck, let's do as before and use multiple accumulators:

\begin{lstlisting}
const int PAR = 2;
ll subprod[PAR] = {1,1};
while (cur + PAR <= lim) {
	rep(i,0,PAR) {
		if ((u32)cur * pinv > plim)
			subprod[i] = subprod[i] * cur % mod;
		cur++;
	}
}
rep(i,0,PAR) prod = prod * subprod[i] % mod;
for (; cur < lim; cur++) {
	if ((u32)cur * pinv > plim)
		prod = prod * cur % mod;
}
\end{lstlisting}

This gives us a (still pretty awful) runtime of 11.848s. Making the divisibility check branchless by doing
\begin{lstlisting}
subprod[i] * ((u32)cur * pinv > plim ? cur : 1) % mod;
\end{lstlisting}
improves it further to 10.933s (10.391s with no divisibility check at all).
Raising \verb@PAR@ further makes performance worse, and turning on -O3 makes no difference.

So... where do we go from here?
We can't turn to SIMD, because there's no SIMD integer division or modulo instruction.
Well, what we \emph{can} do is roll our own modular multiplication.
% There a couple of different techniques for this, usually relying on the modulus remaining constant over many operations.

\subsection*{Barrett reduction}
One way of doing modular reduction quickly is by using Barrett reduction.
The way it works is by writing $a \% b = a - \lfloor a / b \rfloor b$, and approximately computing the value of $\lfloor a / b \rfloor$ as $\lfloor (\lfloor (2^{64} - 1) / b \rfloor \cdot a) / 2^{64} \rfloor$.
We can then apply a final correction if the resulting approximate value of $a \% b$ is outside the range $[0, b)$.
Since $b =$ \verb@mode@ will be constant over many multiplications, we can precompute the value of $\lfloor (2^{64} - 1) / b \rfloor$, turning the work needed for a single modular reduction into just two multiplications, a subtraction and a range correction.
\begin{lstlisting}
typedef uint64_t u64;
typedef __uint128_t u128;
struct Barrett {
	u64 b, m;
	Barrett(u64 b) : b(b), m(-1ULL / b) {}
	u64 reduce(u64 a) {
		u64 q = (u64)((u128(m) * a) >> 64), r = a - q * b;
		return r - b * (r >= b);
	}
};
\end{lstlisting}

Note that while we appear to use 128-bit arithmetic in the \verb@reduce@ function, which seems slow, the x86 instruction set actually has an instruction for \mbox{$64*64\rightarrow128$} bit multiplication, making the computation of \verb@q@ into a single instruction.
(Other instruction sets commonly also have this sort of operation -- it's useful for implementing arbitrary precision integers.)

We can now rewrite the loop:
\begin{lstlisting}
Barrett ba(mod);
...

const int PAR = 16;
u64 subprod[PAR];
rep(i,0,PAR) subprod[i] = 1;
while (cur + PAR <= lim) {
	rep(j,0,PAR) {
		u64 cur2 = (u32)cur * pinv > plim ? cur : 1;
		subprod[j] = ba.reduce(subprod[j] * cur2);
		cur++;
	}
}
rep(i,0,PAR) prod = ba.reduce(prod * subprod[i]);
for (; cur < lim; cur++) {
	u64 cur2 = (u32)cur * pinv > plim ? cur : 1;
	prod = ba.reduce(prod * cur2);
}
\end{lstlisting}
This runs in 2.541s, quite a lot better than before! As a side note, Barrett reduction corresponds roughly to what compilers do to optimize \verb@a % b@ where \verb@b@ is a compile-time constant.

\subsection*{Relaxed Barrett reduction}
The step in the Barrett reduction where we reduce $a$ into $[0, b)$ is nice for giving an answer in canonical form, but it is a bit unnecessarily slow. If we accept working with numbers in the range $[0, 2b)$, we can skip that part and do a reduction only at the end:
\begin{lstlisting}
u64 reduce(u64 a) {
	return a - (u64)((u128(m) * a) >> 64) * b;
}
...

prod %= mod;
\end{lstlisting}
1.746s! Nice. In reference to the side note from before, this is actually \emph{faster} than what the compiler would have produced with the naive version if \verb@mod@ was constant, which is an impressive feat.

\subsection*{Floating-point modmul}


\subsection*{Montgomery multiplication}

SIMD versions (modulo: no, barrett: no, float: yes, montgomery: yes)
\url{https://gist.github.com/simonlindholm/51f88e9626408723cf906c6debd3814b}

\chapter{Example: Fibonacci-ish II}
% concepts: bitsets, using data structures, shuffles
\url{https://codeforces.com/contest/633/problem/H}
problem description. n,m,q <= 30'000, 5s TL. solve in omega(NQ) (265 ms doable)
want to loop over all numbers in order, and process them only if relevant
how to find relevant numbers:
- loop over query range and mark
- loop and check indices (a <= x < b)
- branchless
- bitset RMQ ... or segtree
optimizing the segtree for memory and construction time, and thus perf
shuffle based on bits (\verb@_mm_shuffle_epi8@)
how to read bits from a bitset
lookup tables for popcount using those bits
using small data types, \verb@_mm_madd_epi16@

\chapter{Example: Spelling Correction}
% concepts: weird simd instructions, memory ordering, horizontal reductions, DP
\url{https://kth.kattis.com/problems/kth.adk.spelling}
fast input
char -> byte
transposed checks, no data structures
easy pruning
simd pruning (\verb@_mm_sad_epu8@)
bitparallel edit distance
simd edit distance, log reduction
diagonal edit distance

\chapter{Example: Counting on a Tree}
% concepts: using data structures, counting memory, weird simd instructions, winning factors 2, optimism
\url{https://www.hackerrank.com/contests/university-codesprint/challenges/counting-on-a-tree}
$n \le 10^5, q \le 5\cdot 10^4$, TL 2s (passed in 0.73s), SSE4.1 available on judge
rough math
three parts to each query:
1. walk the path between two nodes
2. write down frequency counts
3. dot product of frequency counts
walking path:
- pointer-chasing: incredibly slow
- split path into smaller parts with power-of-two jumps; now decrease latency by following all paths in parallel. Still rather slow
- HLD; decompose path in $O(log N)$ paths, each of which is consecutive in memory. Fast!
writing down frequency counts: maaybe fast enough
dot product: fast!
try it! hard to optimize further, step 2 is a bottleneck
alternative method: let's combine 1 and 2, and precompute the frequency counts for each node up to the root. Persistent tree
Step 3: needs to compute (a+b-c-d)*(e+f-g-h), recursing down the tree
make leaves big to eliminate that overhead
memory will not fit in L1, and is thus a good proxy. $10^5 \cdot 5\cdot 10^4 \cdot 8 \cdot 4 / 2 = 80$ GB/s, too much
small/large partitioning
\verb@_mm_maddubs_epi16@
handle easy cases (colors that occur once)
same nodes in recursion
$10^5 \cdot 5\cdot 10^4 \cdot 6 / 2 / 2 = 7.5$ GB/s
optimism: halve that
ran in 0.73s

\chapter{Example: Everyone in the Name of Justice}
% concepts: data arrangement, bitslicing, popcount
% [Ynoi2014]人人本着正义之名; chinese text is slow to compile with xelatex and the ctex package
\url{https://www.luogu.com.cn/problem/P5066}
7 types of operations
rough math
shifts are slow (but try it anyway)
counting bits:
- shuffles
- bitslicing (Harley-Seal)
rearranging data
accidental bottlenecks
focus on the innermost loop
L2->L1 idea
minimizing reads/writes by synchronizing operations
(looking at asm to find bad alias analysis)
combining popcounts
\url{https://gist.github.com/simonlindholm/0762dfb382249a1916a7f885183c84b5}
\url{https://gist.github.com/simonlindholm/6a6561bbca9b80af2e46a148791bc53c}

\end{document}
